SBATCH는 사용자가 작성한 스크립트를 스케줄러를 통해 배치 방식으로 작업을 제출하고 출력 결과는 별도의 로그 파일을 통해 확인하는 방식

### 작업 제출 스크립트 작성

```
$ vi job_submit.sh

$ cat job_submit.sh
#!/bin/sh
#SBATCH --job-name=job_test1  ### 작업명
#SBATCH --chdir=/home/admin/temp ### 작업디렉토리
#SBATCH --output=/home/admin/temp/job_test1.log   ### 결과로그파일
#SBATCH --get-user-env  ### 현재 터미널에 로드된 환경설정을 그대로 유지
#SBATCH --nodes=1  ### 작업할당 노드 수
#SBATCH --tasks-per-node=8  ### 작업할당 노드별 코어수
#SBATCH --ntasks=8  ### 작업에 할당된 전체 코어수
#SBATCH --partition=all.q  ### 작업을 실행할 파티션(자원그룹) 지정

srun hostname
env | grep -i -e slurm
```

### 작업 제출

```
$ sbatch job_submit.sh
Submitted batch job 52
```

### 작업 로그 확인

```
$ cat job_test1.log
/var/spool/slurmd/job00053/slurm_script: line 11: .gc/job_test1_pe_hostfile: No such file or directory
/var/spool/slurmd/job00053/slurm_script: line 12: .gc/job_test1_machines: No such file or directory
/var/spool/slurmd/job00053/slurm_script: line 14: .gc/job_test1_gpu_hostfile_53: No such file or directory
master90
master90
master90
master90
master90
master90
master90
master90
SLURM_NODEID=0
SLURM_TASK_PID=886427
SLURM_PRIO_PROCESS=0
SLURM_SUBMIT_DIR=/home/admin
SLURM_PROCID=0
SLURM_JOB_GID=1999
SLURMD_NODENAME=master90
SLURM_TASKS_PER_NODE=8
SLURM_NNODES=1
SLURM_GET_USER_ENV=1
SLURM_NTASKS_PER_NODE=8
SLURM_JOB_NODELIST=master90
SLURM_CLUSTER_NAME=gridcenter
SLURM_NODELIST=master90
SLURM_NTASKS=8
SLURM_JOB_CPUS_PER_NODE=8
SLURM_TOPOLOGY_ADDR=master90
SLURM_WORKING_CLUSTER=gridcenter:192.168.207.90:6817:9728:109
SLURM_JOB_NAME=job_test1
SLURM_JOBID=53
SLURM_CONF=/engrid/enslurm/slurm/slurm.conf
SLURM_NODE_ALIASES=(null)
SLURM_JOB_QOS=prj_admin_qos
SLURM_TOPOLOGY_ADDR_PATTERN=node
SLURM_CPUS_ON_NODE=8
SLURM_JOB_NUM_NODES=1
SLURM_JOB_UID=2001
SLURM_JOB_PARTITION=all.q
SCHED_TYPE=slurm
SLURM_JOB_USER=admin
SLURM_NPROCS=8
SLURM_SUBMIT_HOST=master90
SLURM_JOB_ACCOUNT=prj_admin
SLURM_GTIDS=0
SLURM_JOB_ID=53
SLURM_LOCALID=0
```

### 작업 종료

* scancel <Job id>

```
$ scancel 52
```